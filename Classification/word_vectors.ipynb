{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD VECTORS FEATURES - XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Helio\\Documents\\akira\\ECONO3_TUTORIAIS\\Scrapings\\\\')\n",
    "#os.chdir(r'C:\\Users\\C336682\\Documents\\fake\\IC_FakeNews-master\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('news_concat_01.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indústria brasileira reage com melhora do comé...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A bancarrota de Detroit deixa no ar as pensões...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIB no Brasil cai 0,5% na leitura trimestral, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O órgão supervisor europeu questiona o trabalh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vega Sicilia, a ilusão da escassez</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Label\n",
       "0  Indústria brasileira reage com melhora do comé...      1\n",
       "1  A bancarrota de Detroit deixa no ar as pensões...      1\n",
       "2  PIB no Brasil cai 0,5% na leitura trimestral, ...      1\n",
       "3  O órgão supervisor europeu questiona o trabalh...      1\n",
       "4                Vega Sicilia, a ilusão da escassez       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Helio\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Helio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\Helio\\Documents\\akira\\ECONO3_TUTORIAIS\\embedding\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('word2v_cbow_s300.txt', unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s, model):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow_list = [sent2vec(x, model) for x in data['Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow = pd.DataFrame()\n",
    "\n",
    "for j in range(len(X_w2v_cbow_list)):\n",
    "    X_w2v_cbow[j] = X_w2v_cbow_list[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow = X_w2v_cbow.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for j in range(1,301):\n",
    "    columns.append('feature'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature291</th>\n",
       "      <th>feature292</th>\n",
       "      <th>feature293</th>\n",
       "      <th>feature294</th>\n",
       "      <th>feature295</th>\n",
       "      <th>feature296</th>\n",
       "      <th>feature297</th>\n",
       "      <th>feature298</th>\n",
       "      <th>feature299</th>\n",
       "      <th>feature300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042570</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.074938</td>\n",
       "      <td>-0.013384</td>\n",
       "      <td>0.092475</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.070357</td>\n",
       "      <td>-0.024914</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>-0.077279</td>\n",
       "      <td>-0.017853</td>\n",
       "      <td>-0.006452</td>\n",
       "      <td>-0.017334</td>\n",
       "      <td>-0.106524</td>\n",
       "      <td>-0.065605</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>-0.037649</td>\n",
       "      <td>-0.061136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018268</td>\n",
       "      <td>0.116134</td>\n",
       "      <td>0.088298</td>\n",
       "      <td>-0.064570</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>0.039332</td>\n",
       "      <td>0.083535</td>\n",
       "      <td>0.042699</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>-0.036500</td>\n",
       "      <td>-0.066888</td>\n",
       "      <td>-0.001920</td>\n",
       "      <td>0.123694</td>\n",
       "      <td>-0.045279</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>-0.082070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095050</td>\n",
       "      <td>0.034736</td>\n",
       "      <td>-0.040186</td>\n",
       "      <td>-0.048977</td>\n",
       "      <td>0.070348</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>0.089740</td>\n",
       "      <td>-0.068805</td>\n",
       "      <td>-0.027196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.035696</td>\n",
       "      <td>0.043836</td>\n",
       "      <td>-0.121432</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.087685</td>\n",
       "      <td>0.047486</td>\n",
       "      <td>-0.124016</td>\n",
       "      <td>-0.026470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.099607</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.032469</td>\n",
       "      <td>0.067076</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063799</td>\n",
       "      <td>-0.092861</td>\n",
       "      <td>0.096599</td>\n",
       "      <td>-0.058167</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.069895</td>\n",
       "      <td>0.141475</td>\n",
       "      <td>-0.025738</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>-0.052845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064135</td>\n",
       "      <td>-0.029280</td>\n",
       "      <td>-0.068144</td>\n",
       "      <td>-0.130111</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>-0.007988</td>\n",
       "      <td>0.058953</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002356</td>\n",
       "      <td>0.042550</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>0.034265</td>\n",
       "      <td>-0.035605</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.022376</td>\n",
       "      <td>-0.005851</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>0.033598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.042570  0.012104  0.074938 -0.013384  0.092475 -0.031281 -0.070357   \n",
       "1 -0.018268  0.116134  0.088298 -0.064570  0.011260  0.039332  0.083535   \n",
       "2  0.095050  0.034736 -0.040186 -0.048977  0.070348  0.006896  0.065039   \n",
       "3  0.019449  0.099607 -0.023152  0.035851 -0.018510  0.036798  0.032469   \n",
       "4  0.064135 -0.029280 -0.068144 -0.130111  0.004120 -0.007988  0.058953   \n",
       "\n",
       "   feature8  feature9  feature10     ...      feature291  feature292  \\\n",
       "0 -0.024914 -0.004816   0.013637     ...        0.005758   -0.077279   \n",
       "1  0.042699 -0.000626   0.071059     ...       -0.057831   -0.023792   \n",
       "2  0.089740 -0.068805  -0.027196     ...        0.003375    0.035696   \n",
       "3  0.067076  0.024612   0.009722     ...        0.063799   -0.092861   \n",
       "4  0.000371  0.019016   0.033978     ...       -0.002356    0.042550   \n",
       "\n",
       "   feature293  feature294  feature295  feature296  feature297  feature298  \\\n",
       "0   -0.017853   -0.006452   -0.017334   -0.106524   -0.065605    0.046967   \n",
       "1    0.017571   -0.036500   -0.066888   -0.001920    0.123694   -0.045279   \n",
       "2    0.043836   -0.121432   -0.002649    0.018771    0.087685    0.047486   \n",
       "3    0.096599   -0.058167   -0.003396    0.069895    0.141475   -0.025738   \n",
       "4    0.035945    0.034265   -0.035605    0.000997   -0.022376   -0.005851   \n",
       "\n",
       "   feature299  feature300  \n",
       "0   -0.037649   -0.061136  \n",
       "1    0.013808   -0.082070  \n",
       "2   -0.124016   -0.026470  \n",
       "3    0.007802   -0.052845  \n",
       "4   -0.010347    0.033598  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v_cbow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(data.Label.values)\n",
    "X = X_w2v_cbow.values\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,\n",
    "                                            random_state = 42,\n",
    "                                            test_size = 0.2,\n",
    "                                            stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Helio\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9262800084979818"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [\"Em entrevista, Moro fala em reação, cita risco de morte e dá duríssimo recado aos bandidos so Brasil\",\n",
    "        \"Bolsonaro ganha em disparada na contagem de votos do IBGE\",\n",
    "        \"Lula condenado à 10 anos de cadeia pelo caso do Triplex\",\n",
    "        \"Lula ordena a prisão de Sérgio Moro\",\n",
    "        \"Peixe alienígena encontrado no Rio de Janeiro\",\n",
    "        \"Lula preso amanhã\",\n",
    "        \"Terremoto no Brasil destroi cidades no Alagoas\",\n",
    "        \"Dilma reassumiu a presidência após prisão de Michel Temer\",\n",
    "        \"Michel Temer assume presidência do Brasil\",\n",
    "        \"Toffoli propõe estender restrição ao foro para todas as autoridades\",\n",
    "        \"Intenção de votos em Bolsonaro dispara no mês de Outubro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(noticia, model):\n",
    "    \n",
    "    test=sent2vec(noticia, model)\n",
    "    vector = np.zeros((1,300))\n",
    "    \n",
    "    j=0\n",
    "    for n in test:\n",
    "        vector[0,j] = n\n",
    "        j+=1\n",
    "    \n",
    "\n",
    "    prob = round((clf.predict_proba(vector)[0][0]),4)*100\n",
    "    print(\"A notícia tem \" + str(prob)[:5] + \"% de probalidade de ser falsa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em entrevista, Moro fala em reação, cita risco de morte e dá duríssimo recado aos bandidos so Brasil\n",
      "A notícia tem 79.96% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Bolsonaro ganha em disparada na contagem de votos do IBGE\n",
      "A notícia tem 29.08% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula condenado à 10 anos de cadeia pelo caso do Triplex\n",
      "A notícia tem 21.27% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula ordena a prisão de Sérgio Moro\n",
      "A notícia tem 41.31% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Peixe alienígena encontrado no Rio de Janeiro\n",
      "A notícia tem 42.03% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula preso amanhã\n",
      "A notícia tem 11.71% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Terremoto no Brasil destroi cidades no Alagoas\n",
      "A notícia tem 18.16% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Dilma reassumiu a presidência após prisão de Michel Temer\n",
      "A notícia tem 17.50% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Michel Temer assume presidência do Brasil\n",
      "A notícia tem 14.73% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Toffoli propõe estender restrição ao foro para todas as autoridades\n",
      "A notícia tem 16.00% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Intenção de votos em Bolsonaro dispara no mês de Outubro\n",
      "A notícia tem 78.20% de probalidade de ser falsa.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in news:\n",
    "    print(n)\n",
    "    classify(n, model)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through different word vectors: Word2Vec, Glove, Wang2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors =['word2v_cbow_s300.txt',\n",
    "          'word2v_skip_s300.txt',\n",
    "          'fasttext_cbow_s300.txt',\n",
    "          'fasttext_skip_s300.txt',\n",
    "          'wang2v_cbow_s300.txt',\n",
    "          'wang2v_skip_s300.txt',\n",
    "          'glove_s300.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_vectors = []\n",
    "Keyedvectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in vectors:\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(vector, unicode_errors=\"ignore\")\n",
    "    Keyedvectors.append(model)\n",
    "    \n",
    "    X_word_vector_list = [sent2vec(x, model) for x in data['Title']]\n",
    "    \n",
    "    X_word_vector = pd.DataFrame()\n",
    "\n",
    "    for j in range(len(X_word_vector_list)):\n",
    "        X_word_vector[j] = X_word_vector_list[j]\n",
    "    \n",
    "    X_word_vector = X_word_vector.T\n",
    "    \n",
    "    columns = []\n",
    "    for j in range(1,301):\n",
    "        columns.append('feature'+str(j))\n",
    "    \n",
    "    X_word_vector.columns = columns\n",
    "    \n",
    "    X_word_vectors.append(X_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2v_cbow_s300.txt\n",
      "0.9262800084979818\n",
      "\n",
      "\n",
      "word2v_skip_s300.txt\n",
      "0.9379647333758232\n",
      "\n",
      "\n",
      "fasttext_cbow_s300.txt\n",
      "0.9354153388570214\n",
      "\n",
      "\n",
      "fasttext_skip_s300.txt\n",
      "0.9343530911408541\n",
      "\n",
      "\n",
      "wang2v_cbow_s300.txt\n",
      "0.9335032929679201\n",
      "\n",
      "\n",
      "wang2v_skip_s300.txt\n",
      "0.934777990227321\n",
      "\n",
      "\n",
      "glove_s300.txt\n",
      "0.9326534947949862\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "\n",
    "for el in X_word_vectors:\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y = label_encoder.fit_transform(data.Label.values)\n",
    "    X = el.values\n",
    "\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,\n",
    "                                                random_state = 42,\n",
    "                                                test_size = 0.2,\n",
    "                                                stratify = y) \n",
    "    \n",
    "    clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "    print(vectors[j])\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    print(clf.score(Xtest, Ytest))\n",
    "    print('\\n')\n",
    "    \n",
    "    j+=1\n",
    "    \n",
    "    #BEST: WORD2VEC-SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: TUNE HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORKS WITH WORDVEC-SKIPGRAM FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(vectors[1], unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w2v = [sent2vec(x, model) for x in data['Title']]\n",
    "x_w2v = np.array(x_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(data.Label.values)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(x_w2v, y,\n",
    "                                            random_state = 42,\n",
    "                                            test_size = 0.2,\n",
    "                                            stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_w2v_scl = scl.fit_transform(Xtrain)\n",
    "xtest_w2v_scl = scl.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(Ytrain)\n",
    "ytest_enc = np_utils.to_categorical(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18826, 300), (18826, 2), (4707, 300), (4707, 2))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_w2v_scl.shape, ytrain_enc.shape, xtest_w2v_scl.shape, ytest_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_2 to have shape (3,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-512a51c3f933>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(xtrain_w2v_scl, y=ytrain_enc, batch_size=64, \n\u001b[0;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m           validation_data=(xtest_w2v_scl, ytest_enc))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected activation_2 to have shape (3,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain_w2v_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xtest_w2v_scl, ytest_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
