{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD VECTORS FEATURES - XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(r'C:\\Users\\Helio\\Documents\\akira\\ECONO3_TUTORIAIS\\Scrapings\\\\')\n",
    "os.chdir(r'C:\\Users\\C336682\\Documents\\fake\\IC_FakeNews-master\\data\\new\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('news_concat_01.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Presidente da Croácia de biquíni na praia #'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(data.iloc[22200,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Indústria brasileira reage com melhora do comé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A bancarrota de Detroit deixa no ar as pensões...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>PIB no Brasil cai 0,5% na leitura trimestral, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>O órgão supervisor europeu questiona o trabalh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Vega Sicilia, a ilusão da escassez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                              Title\n",
       "0      1  Indústria brasileira reage com melhora do comé...\n",
       "1      1  A bancarrota de Detroit deixa no ar as pensões...\n",
       "2      1  PIB no Brasil cai 0,5% na leitura trimestral, ...\n",
       "3      1  O órgão supervisor europeu questiona o trabalh...\n",
       "4      1                Vega Sicilia, a ilusão da escassez "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C336682\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\C336682\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(r'C:\\Users\\Helio\\Documents\\akira\\ECONO3_TUTORIAIS\\embedding\\\\')\n",
    "os.chdir(r'C:\\Users\\C336682\\Documents\\fake\\IC_FakeNews-master\\embeddings\\w2v_cbow_s300\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = KeyedVectors.load_word2vec_format('word2v_cbow_s300.txt', unicode_errors=\"ignore\")\n",
    "model_w2v = KeyedVectors.load_word2vec_format('cbow_s300.txt', unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s, model):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    #words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow_list = [sent2vec(x, model) for x in data['Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow = pd.DataFrame()\n",
    "\n",
    "for j in range(len(X_w2v_cbow_list)):\n",
    "    X_w2v_cbow[j] = X_w2v_cbow_list[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow = X_w2v_cbow.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for j in range(1,301):\n",
    "    columns.append('feature'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_cbow.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature291</th>\n",
       "      <th>feature292</th>\n",
       "      <th>feature293</th>\n",
       "      <th>feature294</th>\n",
       "      <th>feature295</th>\n",
       "      <th>feature296</th>\n",
       "      <th>feature297</th>\n",
       "      <th>feature298</th>\n",
       "      <th>feature299</th>\n",
       "      <th>feature300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.042570</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.074938</td>\n",
       "      <td>-0.013384</td>\n",
       "      <td>0.092475</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.070357</td>\n",
       "      <td>-0.024914</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>-0.077279</td>\n",
       "      <td>-0.017853</td>\n",
       "      <td>-0.006452</td>\n",
       "      <td>-0.017334</td>\n",
       "      <td>-0.106524</td>\n",
       "      <td>-0.065605</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>-0.037649</td>\n",
       "      <td>-0.061136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.018268</td>\n",
       "      <td>0.116134</td>\n",
       "      <td>0.088298</td>\n",
       "      <td>-0.064570</td>\n",
       "      <td>0.011260</td>\n",
       "      <td>0.039332</td>\n",
       "      <td>0.083535</td>\n",
       "      <td>0.042699</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>-0.036500</td>\n",
       "      <td>-0.066888</td>\n",
       "      <td>-0.001920</td>\n",
       "      <td>0.123694</td>\n",
       "      <td>-0.045279</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>-0.082070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095050</td>\n",
       "      <td>0.034736</td>\n",
       "      <td>-0.040186</td>\n",
       "      <td>-0.048977</td>\n",
       "      <td>0.070348</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>0.089740</td>\n",
       "      <td>-0.068805</td>\n",
       "      <td>-0.027196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.035696</td>\n",
       "      <td>0.043836</td>\n",
       "      <td>-0.121432</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.087685</td>\n",
       "      <td>0.047486</td>\n",
       "      <td>-0.124016</td>\n",
       "      <td>-0.026470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.099607</td>\n",
       "      <td>-0.023152</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.032469</td>\n",
       "      <td>0.067076</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063799</td>\n",
       "      <td>-0.092861</td>\n",
       "      <td>0.096599</td>\n",
       "      <td>-0.058167</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.069895</td>\n",
       "      <td>0.141475</td>\n",
       "      <td>-0.025738</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>-0.052845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064135</td>\n",
       "      <td>-0.029280</td>\n",
       "      <td>-0.068144</td>\n",
       "      <td>-0.130111</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>-0.007988</td>\n",
       "      <td>0.058953</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.033978</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002356</td>\n",
       "      <td>0.042550</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>0.034265</td>\n",
       "      <td>-0.035605</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.022376</td>\n",
       "      <td>-0.005851</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>0.033598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0  0.042570  0.012104  0.074938 -0.013384  0.092475 -0.031281 -0.070357   \n",
       "1 -0.018268  0.116134  0.088298 -0.064570  0.011260  0.039332  0.083535   \n",
       "2  0.095050  0.034736 -0.040186 -0.048977  0.070348  0.006896  0.065039   \n",
       "3  0.019449  0.099607 -0.023152  0.035851 -0.018510  0.036798  0.032469   \n",
       "4  0.064135 -0.029280 -0.068144 -0.130111  0.004120 -0.007988  0.058953   \n",
       "\n",
       "   feature8  feature9  feature10     ...      feature291  feature292  \\\n",
       "0 -0.024914 -0.004816   0.013637     ...        0.005758   -0.077279   \n",
       "1  0.042699 -0.000626   0.071059     ...       -0.057831   -0.023792   \n",
       "2  0.089740 -0.068805  -0.027196     ...        0.003375    0.035696   \n",
       "3  0.067076  0.024612   0.009722     ...        0.063799   -0.092861   \n",
       "4  0.000371  0.019016   0.033978     ...       -0.002356    0.042550   \n",
       "\n",
       "   feature293  feature294  feature295  feature296  feature297  feature298  \\\n",
       "0   -0.017853   -0.006452   -0.017334   -0.106524   -0.065605    0.046967   \n",
       "1    0.017571   -0.036500   -0.066888   -0.001920    0.123694   -0.045279   \n",
       "2    0.043836   -0.121432   -0.002649    0.018771    0.087685    0.047486   \n",
       "3    0.096599   -0.058167   -0.003396    0.069895    0.141475   -0.025738   \n",
       "4    0.035945    0.034265   -0.035605    0.000997   -0.022376   -0.005851   \n",
       "\n",
       "   feature299  feature300  \n",
       "0   -0.037649   -0.061136  \n",
       "1    0.013808   -0.082070  \n",
       "2   -0.124016   -0.026470  \n",
       "3    0.007802   -0.052845  \n",
       "4   -0.010347    0.033598  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v_cbow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(data.Label.values)\n",
    "X = X_w2v_cbow.values\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,\n",
    "                                            random_state = 42,\n",
    "                                            test_size = 0.2,\n",
    "                                            stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Helio\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9262800084979818"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [\"Em entrevista, Moro fala em reação, cita risco de morte e dá duríssimo recado aos bandidos so Brasil\",\n",
    "        \"Bolsonaro ganha em disparada na contagem de votos do IBGE\",\n",
    "        \"Lula condenado à 10 anos de cadeia pelo caso do Triplex\",\n",
    "        \"Lula ordena a prisão de Sérgio Moro\",\n",
    "        \"Peixe alienígena encontrado no Rio de Janeiro\",\n",
    "        \"Lula preso amanhã\",\n",
    "        \"Terremoto no Brasil destroi cidades no Alagoas\",\n",
    "        \"Dilma reassumiu a presidência após prisão de Michel Temer\",\n",
    "        \"Michel Temer assume presidência do Brasil\",\n",
    "        \"Toffoli propõe estender restrição ao foro para todas as autoridades\",\n",
    "        \"Intenção de votos em Bolsonaro dispara no mês de Outubro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(noticia, model):\n",
    "    \n",
    "    test=sent2vec(noticia, model)\n",
    "    vector = np.zeros((1,300))\n",
    "    \n",
    "    j=0\n",
    "    for n in test:\n",
    "        vector[0,j] = n\n",
    "        j+=1\n",
    "    \n",
    "\n",
    "    prob = round((clf.predict_proba(vector)[0][0]),4)*100\n",
    "    print(\"A notícia tem \" + str(prob)[:5] + \"% de probalidade de ser falsa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Em entrevista, Moro fala em reação, cita risco de morte e dá duríssimo recado aos bandidos so Brasil\n",
      "A notícia tem 79.96% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Bolsonaro ganha em disparada na contagem de votos do IBGE\n",
      "A notícia tem 29.08% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula condenado à 10 anos de cadeia pelo caso do Triplex\n",
      "A notícia tem 21.27% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula ordena a prisão de Sérgio Moro\n",
      "A notícia tem 41.31% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Peixe alienígena encontrado no Rio de Janeiro\n",
      "A notícia tem 42.03% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Lula preso amanhã\n",
      "A notícia tem 11.71% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Terremoto no Brasil destroi cidades no Alagoas\n",
      "A notícia tem 18.16% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Dilma reassumiu a presidência após prisão de Michel Temer\n",
      "A notícia tem 17.50% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Michel Temer assume presidência do Brasil\n",
      "A notícia tem 14.73% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Toffoli propõe estender restrição ao foro para todas as autoridades\n",
      "A notícia tem 16.00% de probalidade de ser falsa.\n",
      "\n",
      "\n",
      "Intenção de votos em Bolsonaro dispara no mês de Outubro\n",
      "A notícia tem 78.20% de probalidade de ser falsa.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in news:\n",
    "    print(n)\n",
    "    classify(n, model)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through different word vectors: Word2Vec, Glove, Wang2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors =['word2v_cbow_s300.txt',\n",
    "          'word2v_skip_s300.txt',\n",
    "          'fasttext_cbow_s300.txt',\n",
    "          'fasttext_skip_s300.txt',\n",
    "          'wang2v_cbow_s300.txt',\n",
    "          'wang2v_skip_s300.txt',\n",
    "          'glove_s300.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_vectors = []\n",
    "Keyedvectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in vectors:\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(vector, unicode_errors=\"ignore\")\n",
    "    Keyedvectors.append(model)\n",
    "    \n",
    "    X_word_vector_list = [sent2vec(x, model) for x in data['Title']]\n",
    "    \n",
    "    X_word_vector = pd.DataFrame()\n",
    "\n",
    "    for j in range(len(X_word_vector_list)):\n",
    "        X_word_vector[j] = X_word_vector_list[j]\n",
    "    \n",
    "    X_word_vector = X_word_vector.T\n",
    "    \n",
    "    columns = []\n",
    "    for j in range(1,301):\n",
    "        columns.append('feature'+str(j))\n",
    "    \n",
    "    X_word_vector.columns = columns\n",
    "    \n",
    "    X_word_vectors.append(X_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2v_cbow_s300.txt\n",
      "0.9262800084979818\n",
      "\n",
      "\n",
      "word2v_skip_s300.txt\n",
      "0.9379647333758232\n",
      "\n",
      "\n",
      "fasttext_cbow_s300.txt\n",
      "0.9354153388570214\n",
      "\n",
      "\n",
      "fasttext_skip_s300.txt\n",
      "0.9343530911408541\n",
      "\n",
      "\n",
      "wang2v_cbow_s300.txt\n",
      "0.9335032929679201\n",
      "\n",
      "\n",
      "wang2v_skip_s300.txt\n",
      "0.934777990227321\n",
      "\n",
      "\n",
      "glove_s300.txt\n",
      "0.9326534947949862\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "\n",
    "for el in X_word_vectors:\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y = label_encoder.fit_transform(data.Label.values)\n",
    "    X = el.values\n",
    "\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y,\n",
    "                                                random_state = 42,\n",
    "                                                test_size = 0.2,\n",
    "                                                stratify = y) \n",
    "    \n",
    "    clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "    print(vectors[j])\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    print(clf.score(Xtest, Ytest))\n",
    "    print('\\n')\n",
    "    \n",
    "    j+=1\n",
    "    \n",
    "    #BEST: WORD2VEC-SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=10, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: TUNE HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORKS WITH WORDVEC-SKIPGRAM FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(vectors[1], unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w2v = [sent2vec(x, model) for x in data['Title']]\n",
    "x_w2v = np.array(x_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(data.Label.values)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(x_w2v, y,\n",
    "                                                random_state = 42,\n",
    "                                                test_size = 0.2,\n",
    "                                                stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_w2v_scl = scl.fit_transform(Xtrain)\n",
    "xtest_w2v_scl = scl.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(Ytrain)\n",
    "ytest_enc = np_utils.to_categorical(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_dim=300, activation='relu', ))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adamax, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20787 samples, validate on 5197 samples\n",
      "Epoch 1/10\n",
      "20787/20787 [==============================] - ETA: 9:21 - loss: 1.0647 - acc: 0.546 - ETA: 37s - loss: 0.9157 - acc: 0.560 - ETA: 19s - loss: 0.8427 - acc: 0.59 - ETA: 13s - loss: 0.7774 - acc: 0.62 - ETA: 9s - loss: 0.7228 - acc: 0.6585 - ETA: 7s - loss: 0.6829 - acc: 0.678 - ETA: 5s - loss: 0.6505 - acc: 0.697 - ETA: 5s - loss: 0.6241 - acc: 0.711 - ETA: 4s - loss: 0.6037 - acc: 0.723 - ETA: 3s - loss: 0.5854 - acc: 0.732 - ETA: 3s - loss: 0.5710 - acc: 0.739 - ETA: 3s - loss: 0.5550 - acc: 0.748 - ETA: 2s - loss: 0.5405 - acc: 0.756 - ETA: 2s - loss: 0.5229 - acc: 0.766 - ETA: 1s - loss: 0.5128 - acc: 0.771 - ETA: 1s - loss: 0.5004 - acc: 0.777 - ETA: 1s - loss: 0.4934 - acc: 0.781 - ETA: 1s - loss: 0.4861 - acc: 0.786 - ETA: 1s - loss: 0.4797 - acc: 0.789 - ETA: 1s - loss: 0.4726 - acc: 0.793 - ETA: 0s - loss: 0.4653 - acc: 0.796 - ETA: 0s - loss: 0.4590 - acc: 0.799 - ETA: 0s - loss: 0.4496 - acc: 0.804 - ETA: 0s - loss: 0.4412 - acc: 0.809 - ETA: 0s - loss: 0.4366 - acc: 0.811 - ETA: 0s - loss: 0.4325 - acc: 0.813 - 3s 168us/step - loss: 0.4291 - acc: 0.8152 - val_loss: 0.1950 - val_acc: 0.9269\n",
      "Epoch 2/10\n",
      "20787/20787 [==============================] - ETA: 0s - loss: 0.3881 - acc: 0.859 - ETA: 1s - loss: 0.2632 - acc: 0.907 - ETA: 1s - loss: 0.2852 - acc: 0.900 - ETA: 1s - loss: 0.2768 - acc: 0.904 - ETA: 1s - loss: 0.2746 - acc: 0.900 - ETA: 1s - loss: 0.2665 - acc: 0.902 - ETA: 1s - loss: 0.2632 - acc: 0.903 - ETA: 1s - loss: 0.2573 - acc: 0.904 - ETA: 1s - loss: 0.2600 - acc: 0.903 - ETA: 1s - loss: 0.2638 - acc: 0.901 - ETA: 1s - loss: 0.2614 - acc: 0.901 - ETA: 1s - loss: 0.2582 - acc: 0.902 - ETA: 0s - loss: 0.2599 - acc: 0.901 - ETA: 0s - loss: 0.2619 - acc: 0.900 - ETA: 0s - loss: 0.2617 - acc: 0.900 - ETA: 0s - loss: 0.2625 - acc: 0.899 - ETA: 0s - loss: 0.2636 - acc: 0.899 - ETA: 0s - loss: 0.2643 - acc: 0.899 - ETA: 0s - loss: 0.2652 - acc: 0.898 - ETA: 0s - loss: 0.2648 - acc: 0.898 - ETA: 0s - loss: 0.2633 - acc: 0.899 - ETA: 0s - loss: 0.2613 - acc: 0.899 - ETA: 0s - loss: 0.2624 - acc: 0.898 - ETA: 0s - loss: 0.2613 - acc: 0.898 - ETA: 0s - loss: 0.2605 - acc: 0.898 - ETA: 0s - loss: 0.2587 - acc: 0.899 - ETA: 0s - loss: 0.2575 - acc: 0.900 - ETA: 0s - loss: 0.2563 - acc: 0.900 - ETA: 0s - loss: 0.2556 - acc: 0.900 - 2s 82us/step - loss: 0.2552 - acc: 0.9007 - val_loss: 0.1719 - val_acc: 0.9346\n",
      "Epoch 3/10\n",
      "20787/20787 [==============================] - ETA: 0s - loss: 0.1730 - acc: 0.937 - ETA: 1s - loss: 0.2851 - acc: 0.892 - ETA: 1s - loss: 0.2695 - acc: 0.898 - ETA: 1s - loss: 0.2529 - acc: 0.905 - ETA: 1s - loss: 0.2410 - acc: 0.908 - ETA: 1s - loss: 0.2390 - acc: 0.910 - ETA: 1s - loss: 0.2375 - acc: 0.911 - ETA: 1s - loss: 0.2367 - acc: 0.912 - ETA: 0s - loss: 0.2397 - acc: 0.910 - ETA: 0s - loss: 0.2380 - acc: 0.911 - ETA: 0s - loss: 0.2339 - acc: 0.913 - ETA: 0s - loss: 0.2295 - acc: 0.914 - ETA: 0s - loss: 0.2307 - acc: 0.912 - ETA: 0s - loss: 0.2313 - acc: 0.912 - ETA: 0s - loss: 0.2301 - acc: 0.913 - ETA: 0s - loss: 0.2303 - acc: 0.912 - ETA: 0s - loss: 0.2289 - acc: 0.913 - ETA: 0s - loss: 0.2272 - acc: 0.913 - ETA: 0s - loss: 0.2267 - acc: 0.914 - ETA: 0s - loss: 0.2264 - acc: 0.914 - ETA: 0s - loss: 0.2262 - acc: 0.913 - ETA: 0s - loss: 0.2265 - acc: 0.913 - ETA: 0s - loss: 0.2251 - acc: 0.914 - ETA: 0s - loss: 0.2244 - acc: 0.913 - ETA: 0s - loss: 0.2243 - acc: 0.913 - ETA: 0s - loss: 0.2257 - acc: 0.913 - ETA: 0s - loss: 0.2253 - acc: 0.913 - ETA: 0s - loss: 0.2240 - acc: 0.914 - ETA: 0s - loss: 0.2235 - acc: 0.914 - 2s 88us/step - loss: 0.2228 - acc: 0.9146 - val_loss: 0.1626 - val_acc: 0.9390\n",
      "Epoch 4/10\n",
      "20787/20787 [==============================] - ETA: 0s - loss: 0.1223 - acc: 0.937 - ETA: 1s - loss: 0.2238 - acc: 0.912 - ETA: 1s - loss: 0.2178 - acc: 0.915 - ETA: 1s - loss: 0.2148 - acc: 0.921 - ETA: 1s - loss: 0.2254 - acc: 0.914 - ETA: 1s - loss: 0.2197 - acc: 0.917 - ETA: 1s - loss: 0.2172 - acc: 0.918 - ETA: 1s - loss: 0.2218 - acc: 0.916 - ETA: 1s - loss: 0.2156 - acc: 0.918 - ETA: 1s - loss: 0.2127 - acc: 0.919 - ETA: 1s - loss: 0.2137 - acc: 0.919 - ETA: 0s - loss: 0.2121 - acc: 0.918 - ETA: 0s - loss: 0.2094 - acc: 0.920 - ETA: 0s - loss: 0.2108 - acc: 0.920 - ETA: 0s - loss: 0.2083 - acc: 0.920 - ETA: 0s - loss: 0.2083 - acc: 0.920 - ETA: 0s - loss: 0.2095 - acc: 0.920 - ETA: 0s - loss: 0.2086 - acc: 0.920 - ETA: 0s - loss: 0.2100 - acc: 0.919 - ETA: 0s - loss: 0.2076 - acc: 0.921 - ETA: 0s - loss: 0.2059 - acc: 0.921 - ETA: 0s - loss: 0.2051 - acc: 0.922 - ETA: 0s - loss: 0.2046 - acc: 0.922 - ETA: 0s - loss: 0.2052 - acc: 0.922 - ETA: 0s - loss: 0.2058 - acc: 0.921 - ETA: 0s - loss: 0.2053 - acc: 0.921 - ETA: 0s - loss: 0.2037 - acc: 0.921 - ETA: 0s - loss: 0.2028 - acc: 0.921 - ETA: 0s - loss: 0.2036 - acc: 0.921 - 2s 77us/step - loss: 0.2042 - acc: 0.9219 - val_loss: 0.1564 - val_acc: 0.9384\n",
      "Epoch 5/10\n",
      "20787/20787 [==============================] - ETA: 1s - loss: 0.1482 - acc: 0.937 - ETA: 1s - loss: 0.2124 - acc: 0.916 - ETA: 1s - loss: 0.2079 - acc: 0.918 - ETA: 1s - loss: 0.2108 - acc: 0.915 - ETA: 1s - loss: 0.2037 - acc: 0.920 - ETA: 1s - loss: 0.1902 - acc: 0.925 - ETA: 1s - loss: 0.1894 - acc: 0.925 - ETA: 1s - loss: 0.1924 - acc: 0.926 - ETA: 1s - loss: 0.1951 - acc: 0.925 - ETA: 1s - loss: 0.1932 - acc: 0.926 - ETA: 1s - loss: 0.1903 - acc: 0.927 - ETA: 1s - loss: 0.1900 - acc: 0.927 - ETA: 0s - loss: 0.1871 - acc: 0.928 - ETA: 0s - loss: 0.1876 - acc: 0.928 - ETA: 0s - loss: 0.1882 - acc: 0.928 - ETA: 0s - loss: 0.1875 - acc: 0.927 - ETA: 0s - loss: 0.1857 - acc: 0.928 - ETA: 0s - loss: 0.1856 - acc: 0.929 - ETA: 0s - loss: 0.1887 - acc: 0.928 - ETA: 0s - loss: 0.1891 - acc: 0.927 - ETA: 0s - loss: 0.1880 - acc: 0.928 - ETA: 0s - loss: 0.1889 - acc: 0.927 - ETA: 0s - loss: 0.1890 - acc: 0.927 - ETA: 0s - loss: 0.1897 - acc: 0.927 - ETA: 0s - loss: 0.1909 - acc: 0.927 - ETA: 0s - loss: 0.1919 - acc: 0.926 - ETA: 0s - loss: 0.1920 - acc: 0.926 - ETA: 0s - loss: 0.1917 - acc: 0.926 - ETA: 0s - loss: 0.1910 - acc: 0.927 - ETA: 0s - loss: 0.1900 - acc: 0.927 - ETA: 0s - loss: 0.1899 - acc: 0.928 - ETA: 0s - loss: 0.1894 - acc: 0.928 - 2s 85us/step - loss: 0.1891 - acc: 0.9282 - val_loss: 0.1526 - val_acc: 0.9388\n",
      "Epoch 6/10\n",
      "20787/20787 [==============================] - ETA: 1s - loss: 0.1708 - acc: 0.906 - ETA: 1s - loss: 0.1790 - acc: 0.932 - ETA: 1s - loss: 0.2057 - acc: 0.920 - ETA: 1s - loss: 0.2110 - acc: 0.923 - ETA: 1s - loss: 0.2018 - acc: 0.924 - ETA: 1s - loss: 0.1952 - acc: 0.926 - ETA: 1s - loss: 0.1957 - acc: 0.926 - ETA: 1s - loss: 0.1950 - acc: 0.927 - ETA: 1s - loss: 0.1914 - acc: 0.928 - ETA: 1s - loss: 0.1894 - acc: 0.927 - ETA: 1s - loss: 0.1901 - acc: 0.926 - ETA: 1s - loss: 0.1918 - acc: 0.925 - ETA: 1s - loss: 0.1899 - acc: 0.926 - ETA: 1s - loss: 0.1876 - acc: 0.927 - ETA: 0s - loss: 0.1885 - acc: 0.926 - ETA: 0s - loss: 0.1861 - acc: 0.927 - ETA: 0s - loss: 0.1829 - acc: 0.928 - ETA: 0s - loss: 0.1838 - acc: 0.928 - ETA: 0s - loss: 0.1844 - acc: 0.928 - ETA: 0s - loss: 0.1842 - acc: 0.927 - ETA: 0s - loss: 0.1832 - acc: 0.928 - ETA: 0s - loss: 0.1828 - acc: 0.928 - ETA: 0s - loss: 0.1816 - acc: 0.929 - ETA: 0s - loss: 0.1824 - acc: 0.929 - ETA: 0s - loss: 0.1817 - acc: 0.929 - ETA: 0s - loss: 0.1822 - acc: 0.929 - ETA: 0s - loss: 0.1813 - acc: 0.930 - ETA: 0s - loss: 0.1809 - acc: 0.930 - ETA: 0s - loss: 0.1806 - acc: 0.930 - 2s 86us/step - loss: 0.1807 - acc: 0.9303 - val_loss: 0.1483 - val_acc: 0.9404\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20787/20787 [==============================] - ETA: 0s - loss: 0.0605 - acc: 0.984 - ETA: 1s - loss: 0.1432 - acc: 0.944 - ETA: 1s - loss: 0.1620 - acc: 0.935 - ETA: 1s - loss: 0.1684 - acc: 0.936 - ETA: 1s - loss: 0.1737 - acc: 0.935 - ETA: 1s - loss: 0.1737 - acc: 0.933 - ETA: 1s - loss: 0.1768 - acc: 0.930 - ETA: 1s - loss: 0.1740 - acc: 0.932 - ETA: 1s - loss: 0.1722 - acc: 0.933 - ETA: 1s - loss: 0.1687 - acc: 0.934 - ETA: 1s - loss: 0.1708 - acc: 0.933 - ETA: 1s - loss: 0.1713 - acc: 0.932 - ETA: 0s - loss: 0.1721 - acc: 0.932 - ETA: 0s - loss: 0.1731 - acc: 0.932 - ETA: 0s - loss: 0.1733 - acc: 0.932 - ETA: 0s - loss: 0.1721 - acc: 0.932 - ETA: 0s - loss: 0.1698 - acc: 0.933 - ETA: 0s - loss: 0.1700 - acc: 0.933 - ETA: 0s - loss: 0.1699 - acc: 0.933 - ETA: 0s - loss: 0.1690 - acc: 0.934 - ETA: 0s - loss: 0.1695 - acc: 0.934 - ETA: 0s - loss: 0.1688 - acc: 0.934 - ETA: 0s - loss: 0.1691 - acc: 0.934 - ETA: 0s - loss: 0.1694 - acc: 0.934 - ETA: 0s - loss: 0.1714 - acc: 0.934 - ETA: 0s - loss: 0.1716 - acc: 0.934 - ETA: 0s - loss: 0.1717 - acc: 0.934 - ETA: 0s - loss: 0.1715 - acc: 0.934 - ETA: 0s - loss: 0.1713 - acc: 0.935 - ETA: 0s - loss: 0.1698 - acc: 0.935 - 2s 83us/step - loss: 0.1706 - acc: 0.9353 - val_loss: 0.1433 - val_acc: 0.9442\n",
      "Epoch 8/10\n",
      "20787/20787 [==============================] - ETA: 2s - loss: 0.1158 - acc: 0.953 - ETA: 1s - loss: 0.1475 - acc: 0.948 - ETA: 1s - loss: 0.1553 - acc: 0.940 - ETA: 1s - loss: 0.1586 - acc: 0.938 - ETA: 1s - loss: 0.1589 - acc: 0.940 - ETA: 1s - loss: 0.1599 - acc: 0.939 - ETA: 1s - loss: 0.1558 - acc: 0.941 - ETA: 1s - loss: 0.1560 - acc: 0.942 - ETA: 1s - loss: 0.1569 - acc: 0.942 - ETA: 1s - loss: 0.1587 - acc: 0.941 - ETA: 1s - loss: 0.1565 - acc: 0.942 - ETA: 1s - loss: 0.1599 - acc: 0.941 - ETA: 0s - loss: 0.1594 - acc: 0.940 - ETA: 0s - loss: 0.1613 - acc: 0.940 - ETA: 0s - loss: 0.1600 - acc: 0.941 - ETA: 0s - loss: 0.1597 - acc: 0.941 - ETA: 0s - loss: 0.1609 - acc: 0.940 - ETA: 0s - loss: 0.1596 - acc: 0.940 - ETA: 0s - loss: 0.1596 - acc: 0.940 - ETA: 0s - loss: 0.1592 - acc: 0.941 - ETA: 0s - loss: 0.1609 - acc: 0.940 - ETA: 0s - loss: 0.1617 - acc: 0.940 - ETA: 0s - loss: 0.1605 - acc: 0.940 - ETA: 0s - loss: 0.1593 - acc: 0.940 - ETA: 0s - loss: 0.1589 - acc: 0.940 - ETA: 0s - loss: 0.1584 - acc: 0.941 - ETA: 0s - loss: 0.1589 - acc: 0.940 - ETA: 0s - loss: 0.1606 - acc: 0.940 - ETA: 0s - loss: 0.1606 - acc: 0.940 - ETA: 0s - loss: 0.1614 - acc: 0.940 - 2s 82us/step - loss: 0.1612 - acc: 0.9403 - val_loss: 0.1390 - val_acc: 0.9465\n",
      "Epoch 9/10\n",
      "20787/20787 [==============================] - ETA: 0s - loss: 0.1263 - acc: 0.937 - ETA: 1s - loss: 0.1577 - acc: 0.933 - ETA: 1s - loss: 0.1571 - acc: 0.934 - ETA: 1s - loss: 0.1555 - acc: 0.937 - ETA: 1s - loss: 0.1535 - acc: 0.940 - ETA: 1s - loss: 0.1594 - acc: 0.938 - ETA: 1s - loss: 0.1631 - acc: 0.936 - ETA: 1s - loss: 0.1602 - acc: 0.938 - ETA: 0s - loss: 0.1609 - acc: 0.938 - ETA: 0s - loss: 0.1598 - acc: 0.939 - ETA: 0s - loss: 0.1585 - acc: 0.939 - ETA: 0s - loss: 0.1602 - acc: 0.939 - ETA: 0s - loss: 0.1603 - acc: 0.938 - ETA: 0s - loss: 0.1601 - acc: 0.939 - ETA: 0s - loss: 0.1583 - acc: 0.940 - ETA: 0s - loss: 0.1572 - acc: 0.940 - ETA: 0s - loss: 0.1570 - acc: 0.940 - ETA: 0s - loss: 0.1579 - acc: 0.940 - ETA: 0s - loss: 0.1573 - acc: 0.940 - ETA: 0s - loss: 0.1561 - acc: 0.941 - ETA: 0s - loss: 0.1560 - acc: 0.941 - ETA: 0s - loss: 0.1564 - acc: 0.941 - ETA: 0s - loss: 0.1551 - acc: 0.942 - ETA: 0s - loss: 0.1539 - acc: 0.942 - ETA: 0s - loss: 0.1550 - acc: 0.942 - ETA: 0s - loss: 0.1554 - acc: 0.942 - ETA: 0s - loss: 0.1542 - acc: 0.942 - ETA: 0s - loss: 0.1543 - acc: 0.942 - 2s 82us/step - loss: 0.1546 - acc: 0.9428 - val_loss: 0.1354 - val_acc: 0.9484\n",
      "Epoch 10/10\n",
      "20787/20787 [==============================] - ETA: 0s - loss: 0.2257 - acc: 0.921 - ETA: 1s - loss: 0.1609 - acc: 0.934 - ETA: 1s - loss: 0.1527 - acc: 0.943 - ETA: 1s - loss: 0.1519 - acc: 0.941 - ETA: 1s - loss: 0.1506 - acc: 0.942 - ETA: 1s - loss: 0.1484 - acc: 0.943 - ETA: 1s - loss: 0.1486 - acc: 0.944 - ETA: 1s - loss: 0.1484 - acc: 0.945 - ETA: 1s - loss: 0.1480 - acc: 0.945 - ETA: 1s - loss: 0.1467 - acc: 0.945 - ETA: 1s - loss: 0.1478 - acc: 0.944 - ETA: 0s - loss: 0.1471 - acc: 0.945 - ETA: 0s - loss: 0.1492 - acc: 0.945 - ETA: 0s - loss: 0.1485 - acc: 0.945 - ETA: 0s - loss: 0.1458 - acc: 0.946 - ETA: 0s - loss: 0.1466 - acc: 0.946 - ETA: 0s - loss: 0.1472 - acc: 0.946 - ETA: 0s - loss: 0.1468 - acc: 0.947 - ETA: 0s - loss: 0.1469 - acc: 0.946 - ETA: 0s - loss: 0.1462 - acc: 0.947 - ETA: 0s - loss: 0.1470 - acc: 0.946 - ETA: 0s - loss: 0.1462 - acc: 0.946 - ETA: 0s - loss: 0.1458 - acc: 0.946 - ETA: 0s - loss: 0.1466 - acc: 0.947 - ETA: 0s - loss: 0.1467 - acc: 0.947 - ETA: 0s - loss: 0.1466 - acc: 0.947 - ETA: 0s - loss: 0.1474 - acc: 0.947 - ETA: 0s - loss: 0.1469 - acc: 0.947 - 2s 81us/step - loss: 0.1472 - acc: 0.9475 - val_loss: 0.1314 - val_acc: 0.9505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x456c6ef0>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_w2v_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=10, verbose=1,\n",
    "          validation_data=(xtest_w2v_scl, ytest_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESTÁ ESTRANHO, CLASSIFICANDO TUDO COMO ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = sent2vec(\"# # # # # #\", model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.zeros((1,300))\n",
    "    \n",
    "j=0\n",
    "for n in vec:\n",
    "    vector[0,j] = n\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14055635, 0.8594436 ]], dtype=float32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13137345122180955, 0.9505483933038291]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest_w2v_scl, ytest_enc, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = [\"Em entrevista, Moro fala em reação, cita risco de morte e dá duríssimo recado aos bandidos so Brasil\",\n",
    "        \"Bolsonaro ganha em disparada na contagem de votos do IBGE\",\n",
    "        \"Lula condenado à 10 anos de cadeia pelo caso do Triplex\",\n",
    "        \"Lula ordena a prisão de Sérgio Moro\",\n",
    "        \"Peixe alienígena encontrado no Rio de Janeiro\",\n",
    "        \"Lula preso amanhã\",\n",
    "        \"Terremoto no Brasil destroi cidades no Alagoas\",\n",
    "        \"Dilma reassumiu a presidência após prisão de Michel Temer\",\n",
    "        \"Michel Temer assume presidência do Brasil\",\n",
    "        \"Toffoli propõe estender restrição ao foro para todas as autoridades\",\n",
    "        \"Intenção de votos em Bolsonaro dispara no mês de Outubro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(noticia, model):\n",
    "    \n",
    "    test=sent2vec(noticia, model)\n",
    "    vector = np.zeros((1,300))\n",
    "    \n",
    "    j=0\n",
    "    for n in test:\n",
    "        vector[0,j] = n\n",
    "        j+=1\n",
    "    \n",
    "\n",
    "    print(model.predict_classes(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for n in news:\n",
    "    classify(n, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tentar NN com TFIDFse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
